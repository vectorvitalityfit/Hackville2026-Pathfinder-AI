<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Voice Navigation Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #000;
            color: #fff;
            overflow: hidden;
            height: 100vh;
        }

        #container {
            position: relative;
            width: 100%;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }

        #video-container {
            position: relative;
            flex: 1;
            background: #000;
            overflow: hidden;
        }

        #video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        #overlay {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            pointer-events: none;
        }

        #status {
            position: absolute;
            top: 20px;
            left: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.8);
            padding: 15px;
            border-radius: 10px;
            font-size: 16px;
            backdrop-filter: blur(10px);
        }

        #response {
            position: absolute;
            bottom: 100px;
            left: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.9);
            padding: 20px;
            border-radius: 15px;
            font-size: 20px;
            font-weight: 600;
            text-align: center;
            backdrop-filter: blur(10px);
            display: none;
        }

        #controls {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(20, 20, 20, 0.95);
            padding: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
            backdrop-filter: blur(10px);
        }
        
        .button-row {
            display: flex;
            justify-content: center;
            gap: 15px;
            width: 100%;
        }

        button {
            background: #007AFF;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 25px;
            font-size: 18px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            min-width: 120px;
        }

        button:active {
            transform: scale(0.95);
            background: #0051D5;
        }

        button:disabled {
            background: #666;
            cursor: not-allowed;
        }

        #voice-btn {
            background: #5856D6;
            font-size: 18px;
            padding: 15px 30px;
        }

        #navigate-btn {
            background: #34C759;
            font-size: 20px;
            padding: 15px 35px;
            box-shadow: 0 4px 20px rgba(52, 199, 89, 0.4);
        }

        #navigate-btn.active {
            background: #FF3B30;
            animation: pulse 1s infinite;
        }

        #voice-btn.recording {
            background: #FF9500;
            animation: pulse 1s infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }

        .status-indicator {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
        }

        .status-connected {
            background: #34C759;
            box-shadow: 0 0 10px #34C759;
        }

        .status-disconnected {
            background: #FF3B30;
        }

        #loading {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: #000;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 1000;
        }

        #loading h1 {
            font-size: 32px;
            margin-bottom: 20px;
        }

        #loading p {
            font-size: 18px;
            color: #666;
        }

        .spinner {
            width: 50px;
            height: 50px;
            border: 4px solid #333;
            border-top-color: #007AFF;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin: 20px;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div id="loading">
        <h1>ðŸ§­ Voice Navigator</h1>
        <div class="spinner"></div>
        <p>Initializing camera...</p>
    </div>

    <div id="container" style="display: none;">
        <div id="video-container">
            <video id="video" autoplay playsinline></video>
            <div id="overlay">
                <div id="status">
                    <span class="status-indicator status-disconnected" id="status-indicator"></span>
                    <span id="status-text">Connecting...</span>
                </div>
                <div id="response"></div>
            </div>
        </div>

        <div id="controls">
            <div class="button-row">
                <button id="navigate-btn" disabled>ðŸ§­ Start Navigation</button>
                <button id="voice-btn" disabled>ðŸŽ¤ Ask Question</button>
            </div>
            <div class="button-row">
                <button id="test-audio-btn" style="background: #555; font-size: 14px; padding: 10px 20px;">Test Audio</button>
            </div>
        </div>
    </div>

    <script>
        const API_URL = window.location.origin;
        let mediaRecorder;
        let audioChunks = [];
        let isProcessing = false;
        let stream = null;
        let isNavigating = false;
        let navigationInterval = null;
        let audioQueue = [];
        let isPlayingAudio = false;
        let globalAudioElement = null;
        let lastObstacleState = null; // Track previous obstacles to avoid repeating

        // Elements
        const loading = document.getElementById('loading');
        const container = document.getElementById('container');
        const video = document.getElementById('video');
        const navigateBtn = document.getElementById('navigate-btn');
        const voiceBtn = document.getElementById('voice-btn');
        const testAudioBtn = document.getElementById('test-audio-btn');
        const statusText = document.getElementById('status-text');
        const statusIndicator = document.getElementById('status-indicator');
        const responseDiv = document.getElementById('response');
        
        let lastAudioData = null; // Store last audio for testing

        // Initialize camera
        async function initCamera() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment', width: 640, height: 480 },
                    audio: false
                });
                video.srcObject = stream;
                
                loading.style.display = 'none';
                container.style.display = 'flex';
                
                updateStatus('Ready - Tap screen to enable audio', true);
                navigateBtn.disabled = false;
                voiceBtn.disabled = false;
                
                // Create global audio element for continuous playback
                globalAudioElement = new Audio();
                globalAudioElement.volume = 1.0;
                globalAudioElement.muted = false;
                
                // Unlock audio on first user interaction
                document.body.addEventListener('click', unlockAudio, { once: true });
                document.body.addEventListener('touchstart', unlockAudio, { once: true });
                
            } catch (err) {
                console.error('Camera error:', err);
                loading.querySelector('p').textContent = 'Camera access denied. Please allow camera permission.';
            }
        }

        // Unlock audio context (required for iOS/mobile browsers)
        async function unlockAudio() {
            console.log('=== UNLOCKING AUDIO ===');
            updateStatus('Audio enabled - Ready!', true);
            
            // Create and play silent audio to unlock
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const buffer = audioContext.createBuffer(1, 1, 22050);
            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);
            source.start(0);
            
            console.log('Audio context state:', audioContext.state);
            
            // Also try playing silent mp3
            const silentAudio = new Audio('data:audio/mpeg;base64,//uQxAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAACcQCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICA//////////////////////////////////////////////////////////////////8AAABhTEFNRTMuMTAwA6oAAAAAAAAAABQ4JAMGQgAAOAAAcQBjjg5QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//uQxAAADwABLRmAACAAADSAAAAETEFNRTMuMTAwVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVf/7kMQpgAAAaQAAAAgAAA0gAAABFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVQ==');
            silentAudio.volume = 0.01;
            
            try {
                await silentAudio.play();
                console.log('âœ… Silent audio played - audio unlocked!');
                
                // Wait a moment then greet
                setTimeout(() => {
                    speak("Hello. Tap the button to speak.");
                }, 500);
            } catch (e) {
                console.log('Silent audio failed:', e);
                console.log('Audio might still work, trying anyway...');
            }
        }

        // Update status
        function updateStatus(text, connected = false) {
            statusText.textContent = text;
            if (connected) {
                statusIndicator.className = 'status-indicator status-connected';
            } else {
                statusIndicator.className = 'status-indicator status-disconnected';
            }
        }

        // Show response
        function showResponse(text) {
            responseDiv.textContent = text;
            responseDiv.style.display = 'block';
            setTimeout(() => {
                responseDiv.style.display = 'none';
            }, 5000);
        }

        // Unlock audio context (required for iOS/mobile browsers)
        async function unlockAudio() {
            console.log('=== UNLOCKING AUDIO ===');
            updateStatus('Audio enabled - Ready!', true);
            
            // Create and play silent audio to unlock
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const buffer = audioContext.createBuffer(1, 1, 22050);
            const source = audioContext.createBufferSource();
            source.buffer = buffer;
            source.connect(audioContext.destination);
            source.start(0);
            
            console.log('Audio context state:', audioContext.state);
            
            // Test the global audio element
            const silentAudio = new Audio('data:audio/mpeg;base64,//uQxAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAACcQCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICA//////////////////////////////////////////////////////////////////8AAABhTEFNRTMuMTAwA6oAAAAAAAAAABQ4JAMGQgAAOAAAcQBjjg5QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//uQxAAADwABLRmAACAAADSAAAAETEFNRTMuMTAwVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVf/7kMQpgAAAaQAAAAgAAA0gAAABFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVQ==');
            silentAudio.volume = 0.01;
            
            try {
                await silentAudio.play();
                console.log('âœ… Silent audio played - audio unlocked!');
                
                // Wait a moment then greet
                setTimeout(() => {
                    queueAudio("Navigation ready. Tap start navigation for continuous guidance.");
                }, 500);
            } catch (e) {
                console.log('Silent audio failed:', e);
                console.log('Audio might still work, trying anyway...');
            }
        }

        // Navigation mode toggle
        navigateBtn.addEventListener('click', () => {
            if (isNavigating) {
                stopNavigation();
            } else {
                startNavigation();
            }
        });

        function startNavigation() {
            isNavigating = true;
            navigateBtn.classList.add('active');
            navigateBtn.style.background = '#FF3B30';
            navigateBtn.textContent = 'â¹ï¸ Stop Navigation';
            updateStatus('Navigating - Monitoring surroundings...', true);
            
            speakImmediate("Navigation active.").then(() => {
                // Start monitoring after greeting
                monitorSurroundings();
                navigationInterval = setInterval(() => {
                    if (!isProcessing) {
                        monitorSurroundings();
                    }
                }, 2000);
            });
        }

        function stopNavigation() {
            isNavigating = false;
            navigateBtn.classList.remove('active');
            navigateBtn.style.background = '#34C759';
            navigateBtn.textContent = 'ðŸ§­ Start Navigation';
            updateStatus('Navigation stopped', true);
            
            if (navigationInterval) {
                clearInterval(navigationInterval);
                navigationInterval = null;
            }
            
            lastObstacleState = null; // Reset obstacle tracking
            queueAudio("Navigation stopped.");
        }

        async function monitorSurroundings() {
            if (!isNavigating || isProcessing) return;
            
            isProcessing = true;
            
            try {
                console.log('=== MONITORING SURROUNDINGS ===');
                
                // Capture frame
                const canvas = document.createElement('canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(video, 0, 0);
                const imageBlob = await new Promise(resolve => canvas.toBlob(resolve, 'image/jpeg'));

                // Vision analysis
                const visionForm = new FormData();
                visionForm.append('file', imageBlob, 'frame.jpg');

                const visionResp = await fetch(`${API_URL}/vision/analyze-frame`, {
                    method: 'POST',
                    body: visionForm
                });

                const visionData = await visionResp.json();
                
                // Analyze obstacles and provide directional guidance
                const guidance = analyzeObstaclesAndGuide(visionData);
                
                if (guidance) {
                    console.log('Guidance:', guidance);
                    showResponse(guidance);
                    speakImmediate(guidance); // Use immediate playback instead of queue
                }

            } catch (err) {
                console.error('Monitor error:', err);
            } finally {
                isProcessing = false;
            }
        }

        function analyzeObstaclesAndGuide(visionData) {
            const objects = visionData.objects || [];
            
            if (objects.length === 0) {
                // Clear path
                if (lastObstacleState !== 'clear') {
                    lastObstacleState = 'clear';
                    return 'Path clear.';
                }
                return null; // Don't repeat "clear"
            }

            // Categorize obstacles by position
            let centerObstacles = [];
            let leftObstacles = [];
            let rightObstacles = [];

            objects.forEach(obj => {
                const pos = obj.position?.toLowerCase() || '';
                if (pos.includes('center') || pos.includes('middle')) {
                    centerObstacles.push(obj);
                } else if (pos.includes('left')) {
                    leftObstacles.push(obj);
                } else if (pos.includes('right')) {
                    rightObstacles.push(obj);
                } else {
                    // Default to center if position unclear
                    centerObstacles.push(obj);
                }
            });

            // Generate short, actionable guidance
            let guidance = '';

            if (centerObstacles.length > 0) {
                const obj = centerObstacles[0];
                const name = obj.name || 'obstacle';
                
                // Determine which side is clearer
                if (leftObstacles.length < rightObstacles.length) {
                    guidance = `${name} ahead. Move left.`;
                } else if (rightObstacles.length < leftObstacles.length) {
                    guidance = `${name} ahead. Move right.`;
                } else {
                    guidance = `${name} ahead. Stop.`;
                }
            } else if (leftObstacles.length > 0 && rightObstacles.length > 0) {
                guidance = 'Obstacles both sides. Move carefully.';
            } else if (leftObstacles.length > 0) {
                const obj = leftObstacles[0];
                guidance = `${obj.name || 'Object'} on left. Move right.`;
            } else if (rightObstacles.length > 0) {
                const obj = rightObstacles[0];
                guidance = `${obj.name || 'Object'} on right. Move left.`;
            }

            // Only speak if state changed
            const currentState = JSON.stringify({center: centerObstacles.length, left: leftObstacles.length, right: rightObstacles.length});
            if (lastObstacleState === currentState) {
                return null; // Same situation, don't repeat
            }
            
            lastObstacleState = currentState;
            return guidance;
        }

        // Audio queue system for continuous playback
        function queueAudio(text) {
            console.log('Queueing audio:', text);
            audioQueue.push(text);
            if (!isPlayingAudio) {
                playNextAudio();
            }
        }

        async function playNextAudio() {
            if (audioQueue.length === 0) {
                isPlayingAudio = false;
                console.log('Audio queue empty');
                return;
            }

            isPlayingAudio = true;
            const text = audioQueue.shift();
            
            console.log('Playing from queue:', text);
            
            try {
                await speakContinuous(text);
            } catch (err) {
                console.error('Audio queue error:', err);
            }
            
            // Play next after a short pause
            setTimeout(() => playNextAudio(), 1000);
        }

        async function speakContinuous(text) {
            try {
                console.log('=== CONTINUOUS TTS START ===');
                console.log('Speaking continuous:', text);
                
                const response = await fetch(`${API_URL}/speech/speak`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text })
                });

                if (!response.ok) {
                    throw new Error(`TTS failed: ${response.status}`);
                }

                const data = await response.json();
                console.log('Got audio data, length:', data.audio_base64?.length);
                
                if (!data.audio_base64) {
                    throw new Error('No audio data received');
                }
                
                lastAudioData = data.audio_base64;
                
                // Use global audio element
                globalAudioElement.src = 'data:audio/mpeg;base64,' + data.audio_base64;
                globalAudioElement.preload = 'auto';
                globalAudioElement.load();
                
                console.log('Playing audio now...');
                
                // Wait for audio to finish
                await new Promise((resolve, reject) => {
                    globalAudioElement.onended = () => {
                        console.log('Audio finished playing');
                        resolve();
                    };
                    globalAudioElement.onerror = (e) => {
                        console.error('Audio error:', e);
                        reject(e);
                    };
                    globalAudioElement.play()
                        .then(() => console.log('âœ… Playback started'))
                        .catch(reject);
                });
                
                console.log('=== CONTINUOUS TTS END ===');
                
            } catch (err) {
                console.error('Continuous speech error:', err);
            }
        }

        // Immediate speak for real-time guidance (interrupts if needed)
        async function speakImmediate(text) {
            try {
                console.log('=== IMMEDIATE SPEAK ===', text);
                
                // Stop current audio if playing
                if (globalAudioElement && !globalAudioElement.paused) {
                    globalAudioElement.pause();
                    globalAudioElement.currentTime = 0;
                }
                
                const response = await fetch(`${API_URL}/speech/speak`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text })
                });

                if (!response.ok) {
                    throw new Error(`TTS failed: ${response.status}`);
                }

                const data = await response.json();
                
                if (!data.audio_base64) {
                    throw new Error('No audio data received');
                }
                
                lastAudioData = data.audio_base64;
                
                // Play immediately
                globalAudioElement.src = 'data:audio/mpeg;base64,' + data.audio_base64;
                globalAudioElement.load();
                await globalAudioElement.play();
                console.log('âœ… Immediate playback started');
                
            } catch (err) {
                console.error('Immediate speech error:', err);
            }
        }

        // Voice question (single interaction)
        voiceBtn.addEventListener('click', async () => {
            if (isProcessing) return;

            if (!mediaRecorder || mediaRecorder.state === 'inactive') {
                startRecording();
            } else {
                stopRecording();
            }
        });

        async function startRecording() {
            try {
                const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(audioStream);
                audioChunks = [];

                mediaRecorder.ondataavailable = (e) => {
                    audioChunks.push(e.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    await processVoiceCommand(audioBlob);
                    audioStream.getTracks().forEach(track => track.stop());
                };

                mediaRecorder.start();
                voiceBtn.classList.add('recording');
                voiceBtn.textContent = 'â¹ï¸ Stop';
                updateStatus('Listening to your question...', true);

                // Auto-stop after 5 seconds
                setTimeout(() => {
                    if (mediaRecorder.state === 'recording') {
                        stopRecording();
                    }
                }, 5000);
            } catch (err) {
                console.error('Microphone error:', err);
                updateStatus('Microphone access denied', false);
            }
        }

        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                voiceBtn.classList.remove('recording');
                voiceBtn.textContent = 'ðŸŽ¤ Ask Question';
                updateStatus(isNavigating ? 'Navigating...' : 'Processing...', true);
            }
        }

        async function processVoiceCommand(audioBlob) {
            isProcessing = true;
            voiceBtn.disabled = true;

            try {
                console.log('=== PROCESSING VOICE COMMAND ===');
                
                // Pre-create audio element to keep user interaction context
                const audioElement = new Audio();
                audioElement.volume = 1.0;
                audioElement.muted = false;
                
                // Capture frame
                const canvas = document.createElement('canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(video, 0, 0);
                const imageBlob = await new Promise(resolve => canvas.toBlob(resolve, 'image/jpeg'));

                console.log('Frame captured');

                // Send voice command
                const formData = new FormData();
                formData.append('file', audioBlob, 'audio.webm');

                console.log('Sending to /command/interpret-voice...');
                const cmdResponse = await fetch(`${API_URL}/command/interpret-voice`, {
                    method: 'POST',
                    body: formData
                });

                const cmdData = await cmdResponse.json();
                const intent = cmdData.intent;
                const destination = cmdData.destination;

                console.log('Command processed:', cmdData);
                showResponse(`You said: ${cmdData.transcription || 'Processing...'}`);

                // Process based on intent
                if (intent === 'NAVIGATE_TO_DESTINATION' && destination) {
                    console.log('Starting navigation to:', destination);
                    const navResp = await fetch(`${API_URL}/navigation/start`, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ destination })
                    });
                    
                    const navText = `Navigating to ${destination}. Starting continuous guidance.`;
                    console.log('Speaking navigation start:', navText);
                    showResponse(navText);
                    queueAudio(navText);
                    
                    // Auto-start continuous navigation
                    if (!isNavigating) {
                        startNavigation();
                    }
                    
                } else {
                    console.log('Analyzing scene with intent:', intent);
                    // Get scene description
                    await analyzeScene(imageBlob, intent, audioElement);
                }

                console.log('=== VOICE COMMAND COMPLETE ===');

            } catch (err) {
                console.error('Processing error:', err);
                updateStatus('Error processing command', false);
                showResponse(`Error: ${err.message}`);
            } finally {
                isProcessing = false;
                voiceBtn.disabled = false;
                updateStatus(isNavigating ? 'Navigating - Monitoring...' : 'Ready', true);
            }
        }

        async function analyzeScene(imageBlob, intent = 'describe_once', audioElement = null) {
            try {
                // Vision analysis
                const visionForm = new FormData();
                visionForm.append('file', imageBlob, 'frame.jpg');

                const visionResp = await fetch(`${API_URL}/vision/analyze-frame`, {
                    method: 'POST',
                    body: visionForm
                });

                const visionData = await visionResp.json();

                // Brain processing
                const brainResp = await fetch(`${API_URL}/brain/describe`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        scene_description: visionData.scene_description || '',
                        objects: visionData.objects || [],
                        destination: 'current location',
                        user_context: 'Describe surroundings.',
                        intent: intent
                    })
                });

                const brainData = await brainResp.json();
                const text = brainData.speech_text;

                console.log('Brain response:', text);
                showResponse(text);
                
                // FORCE speak - this is the critical call
                console.log('!!! CALLING SPEAK NOW !!!');
                if (audioElement) {
                    await speakWithElement(text, audioElement);
                } else {
                    await speak(text);
                }
                console.log('!!! SPEAK COMPLETED !!!');

            } catch (err) {
                console.error('Analysis error:', err);
                showResponse('Analysis failed - check console');
                // Even on error, try to speak
                if (audioElement) {
                    await speakWithElement('Sorry, something went wrong.', audioElement);
                } else {
                    await speak('Sorry, something went wrong.');
                }
            }
        }

        // New function that uses a pre-created audio element (keeps user gesture context)
        async function speakWithElement(text, audioElement) {
            try {
                console.log('=== TTS START (WITH ELEMENT) ===');
                console.log('Text to speak:', text);
                
                const response = await fetch(`${API_URL}/speech/speak`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text })
                });

                if (!response.ok) {
                    throw new Error(`TTS failed: ${response.status}`);
                }

                const data = await response.json();
                console.log('Audio data received, length:', data.audio_base64?.length);
                
                if (!data.audio_base64) {
                    throw new Error('No audio data received');
                }
                
                // Store for test button
                lastAudioData = data.audio_base64;
                
                // Use the pre-created audio element to maintain user gesture
                audioElement.src = 'data:audio/mpeg;base64,' + data.audio_base64;
                audioElement.preload = 'auto';
                
                console.log('Audio element configured, attempting to play...');
                
                // Add event listeners
                audioElement.addEventListener('play', () => {
                    console.log('Audio STARTED playing');
                }, { once: true });
                
                audioElement.addEventListener('ended', () => {
                    console.log('Audio FINISHED');
                }, { once: true });
                
                audioElement.addEventListener('error', (e) => {
                    console.error('Audio playback ERROR:', e, audioElement.error);
                    showResponse('âš ï¸ Audio playback failed');
                }, { once: true });
                
                // Try to play (this should work because we're in user gesture context)
                console.log('Attempting audio.play()...');
                await audioElement.play();
                console.log('âœ… audio.play() succeeded');
                
                console.log('=== TTS END ===');
                
            } catch (err) {
                console.error('Speech error:', err);
                showResponse(`Speech error: ${err.message}`);
            }
        }

        // Original speak function (for test button and other cases)
        async function speak(text) {
            try {
                console.log('=== TTS START ===');
                console.log('Text to speak:', text);
                
                const response = await fetch(`${API_URL}/speech/speak`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text })
                });

                if (!response.ok) {
                    throw new Error(`TTS failed: ${response.status}`);
                }

                const data = await response.json();
                console.log('Audio data received, length:', data.audio_base64?.length);
                
                if (!data.audio_base64) {
                    throw new Error('No audio data received');
                }
                
                // Store for test button
                lastAudioData = data.audio_base64;
                
                // Create audio element with explicit settings
                const audio = new Audio();
                audio.src = 'data:audio/mpeg;base64,' + data.audio_base64;
                audio.preload = 'auto';
                audio.volume = 1.0;
                audio.muted = false;
                
                console.log('Audio element created');
                
                // Try to play
                console.log('Attempting audio.play()...');
                const playPromise = audio.play();
                
                if (playPromise !== undefined) {
                    await playPromise;
                    console.log('âœ… audio.play() succeeded');
                } else {
                    console.log('Play promise is undefined');
                }
                
                console.log('=== TTS END ===');
                
            } catch (err) {
                console.error('Speech error:', err);
                showResponse(`Speech error: ${err.message}`);
            }
        }

        // Start on load
        window.addEventListener('load', initCamera);

        // Test audio button
        testAudioBtn.addEventListener('click', async () => {
            console.log('=== TEST AUDIO ===');
            if (lastAudioData) {
                const audio = new Audio('data:audio/mpeg;base64,' + lastAudioData);
                audio.volume = 1.0;
                try {
                    await audio.play();
                    console.log('âœ… Test audio played successfully!');
                    showResponse('âœ… Audio is working!');
                } catch (e) {
                    console.error('âŒ Test audio failed:', e);
                    showResponse('âŒ Audio blocked: ' + e.message);
                }
            } else {
                // Test with simple text
                await speak('Testing audio. Can you hear me?');
            }
        });

        // Keep screen awake
        if ('wakeLock' in navigator) {
            navigator.wakeLock.request('screen').catch(() => {});
        }
    </script>
</body>
</html>
